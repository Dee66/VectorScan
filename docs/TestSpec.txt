VectorScan – Full Test Strategy & Coverage Specification (For Copilot)

Purpose:
This document defines the complete test plan for the VectorScan CLI, including test categories, expected behaviors, fixtures, golden outputs, error handling, Terraform integration tests, IAM drift scoring validation, and end-to-end flows.
Copilot should treat this as authoritative.

1. Test Philosophy

VectorScan enforces Zero-Trust and FinOps guardrails on Terraform plans.
Testing must ensure:

Deterministic behavior

Correct exit codes

Correct validation logic

Safe error handling

Accurate JSON/YAML output

No network calls unless explicitly enabled

Offline mode (VSCAN_OFFLINE=1) disables telemetry, lead capture, Terraform auto-downloads, and StatsD without altering CLI output

Repeatable results across environments

Compatibility with Python 3.9–3.12

Every failure must be reproducible via a fixture.

2. Test Folder Structure

Copilot must generate and maintain this structure:

tests/
  fixtures/
    tfplan_pass.json
    tfplan_fail.json
    tfplan_invalid.json
    tfplan_iam_drift.json
    tfplan_missing_tags.json
    tfplan_no_encryption.json
  golden/
    pass_output.json
    fail_output.json
    iam_drift_output.json
    audit_ledger.yaml
  snapshots/
    test_snapshots.py
  test_cli.py
  test_end_to_end.py
  test_json_output.py
  test_iam_drift.py
  test_audit_ledger.py
  test_error_handling.py
  test_lead_capture.py
  test_terraform_integration.py


Copilot should auto-generate any missing files with placeholders.

3. Unit Test Areas
3.1 CLI Entry Point

Tests must confirm:

correct parsing of arguments

correct routing of flags

correct exit codes

correct detection of missing files

correct detection of invalid JSON

correct default behavior (no flags)

Required assertions:
exit_code == 0 for valid pass plan
exit_code == 3 for violations
exit_code == 2 for missing/invalid JSON
exit_code == 4 for policy pack load errors (missing/corrupted bundled policies)
exit_code == 5 for Terraform test failures (tests executed but reported FAIL)
exit_code == 6 for Terraform automation errors (download/version/exec issues)

4. Functional Tests
4.1 Encryption Mandate (P-SEC-001)

Fixture: fixtures/tfplan_no_encryption.json

Assertions:

violation list contains encryption policy

compliance score deducted correctly

human output includes explicit error message

JSON output includes structured violation block

4.2 Mandatory Tagging (P-FIN-001)

Fixture: fixtures/tfplan_missing_tags.json

Assertions:

missing CostCenter triggers FAIL

missing Project triggers FAIL

violation metadata includes resource address

JSON output shows affected resource count

5. IAM Drift Tests

Fixture: fixtures/tfplan_iam_drift.json

Tests should validate:

drift report generated

risky actions detected

severity map included

penalty deduction applied only when flag set

JSON output contains:

{
  "iam_drift_report": {...},
  "metrics": {
    "iam_drift": <float>,
    "compliance_score": <float>
  }
}

6. Audit Ledger Tests

Fixture: fixtures/tfplan_iam_drift.json
Golden: golden/audit_ledger.yaml

Tests must:

generate deterministic ledger

verify required keys:

environment
scan_timestamp
input_file
violations[]
iam_drift
evidence[]
terraform_test_results (optional)


validate YAML structure

ensure file output works

ensure directory creation works

7. JSON Output Tests

Golden files required:

pass_output.json

fail_output.json

iam_drift_output.json

Assertions:

The CLI emits valid JSON

All keys exist (violations, metrics, timestamp, etc)

Output matches golden files (stable schema)

Schema documentation automation:
- Run `python3 scripts/generate_schema_docs.py` whenever the JSON structure changes to regenerate `docs/output_schema.md` from live CLI output.
- `tests/unit/test_generate_schema_docs_unit.py` ensures the generator keeps producing markdown with the key fields (`status`, `metrics.*`, IAM drift) so CI fails if the schema doc falls out of sync.

8. Error Handling Tests

Fixtures:

tfplan_invalid.json (broken JSON)

nonexistent file path

missing permissions (use monkeypatch)

Assertions:

graceful CLI messages

correct exit code

never throws unhandled exceptions

9. Lead Capture Tests

No network calls unless --endpoint explicitly provided.

Tests must:

9.1 Local Capture

file is written to tools/vectorscan/captures/

JSON contains email + result

timestamp exists

9.2 Remote Capture

Use pytest monkeypatch + temporary mock server:

mock HTTP endpoint returns 200

verify POST payload

verify correct behavior on server error (400/500)

10. Terraform Integration Tests

These tests run only if:

Terraform CLI >= 1.13.5 is detected

or VSCAN_ALLOW_TERRAFORM_DOWNLOAD=1 (legacy: VSCAN_TERRAFORM_AUTO_DOWNLOAD=1) with network access allowed

Tests must:

verify auto-download logic

verify detection of local Terraform

test behavior in CI

verify truncated stdout/stderr logs

validate that results appear under:

"terraform_tests": {
  "status": "PASS/FAIL",
  "version": "...",
  "binary_source": "...",
  "stdout": "...",
  "stderr": "..."
}


Mocking may be used where needed.

11. End-to-End Flow Tests

These simulate the entire user workflow.

Scenario A  -  PASS

run CLI on pass fixture

expect exit code 0

verify human output matches expected template

verify JSON matches golden file

Scenario B  -  FAIL

run CLI on failing fixture

expect exit code 3

verify violations listed clearly

JSON matches golden

Scenario C  -  IAM Drift With Penalty

run CLI with --iam-drift-penalty 35

compliance score reduces correctly

Scenario D  -  Audit Ledger

run CLI with -o audit.yaml

file matches golden ledger

12. Performance Tests

Copilot should create simple tests verifying:

processing time < 200ms for 100KB plan

no memory leaks (use tracemalloc)

large plan (2–5 MB) still succeeds

13. Static Analysis

Copilot should enforce:

mypy type checks

flake8 or ruff

black formatting

Include this in CI.

14. CI/CD Requirements

CI must run:

all pytest suites

Terraform tests (if enabled)

linting

type checks

golden file comparison

15. What Copilot Should Generate Next

Tell Copilot:

Generate all test files under tests/ matching this specification,
including fixtures, golden outputs, pytest tests, and helper utilities.
For any missing internal APIs, generate stubs that match expected behavior.


Additional

1. Add Detailed Schema Validation

Right now, the golden comparison ensures output stability  -  but not structural correctness.

Add jsonschema tests:

tests/schema/test_json_schema_validation.py


Define schema.json and enforce:

required keys

allowed types

no unknown fields

stable order

This makes it impossible for regressions to silently ship.

⭐ 2. Expand Terraform Chaos Tests

Add:

corrupted .terraform folders

plans missing provider blocks

huge IAM policies (>30 KB)

invalid JSON in the IAM policy document

These are extremely common real-world Terraform bugs.

⭐ 3. Add Log Redaction Tests

You added Secrets Redaction  -  now add tests that ensure:

no email appears in logs

no API keys

no partial secrets

no dotenv contents

no AWS creds

This is CRITICALLY important for trust.

⭐ 4. Add “Replay Attack” Test for Deterministic Output

Ensure running:

vectorscan --json tfplan.json > output1.json
vectorscan --json tfplan.json > output2.json
diff output1.json output2.json


produces no drift under deterministic mode.

You already do this  -  but add a STRICT golden test for double verification.

⭐ 5. Negative Test: corrupted .zip bundle WITH partial extraction

Test that:

partial extraction

truncated files

missing policy pack

missing manifest

invalid sbom

missing README

ALL produce graceful errors.

⭐ 6. Test for “CLI invoked from a different directory”

Many users run:

~/Downloads/vectorscan/vectorscan.py xxx.json


Ensure relative paths inside the repo don’t break.





Additional:
Add Tests For:

Remediation block correctness

examples included

stable fields

Policy Preview Mode

ensure the preview doesn’t run illegal paid logic

generate synthetic violations/templates only

Plan Diff Tests

pass plan with changes

golden diff output

deterministic ordering

Explain Mode Tests

stable textual output

deterministic ordering

golden file

Resource Drilldown Tests

correct resource path extraction

HCL mapping accuracy

fails gracefully on unknown resources

Plan Summary Tests

resource_count, module_count, provider list

large plan stability

Interactive Mode (if added)

snapshot-based text UI tests

run headless in CI

GitHub Action Mode

stable non-color JSON output

correct exit codes

Terraform Context Extraction

region detection

provider detection

resource category breakdown